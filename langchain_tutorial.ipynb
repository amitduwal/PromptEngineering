{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: openai in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: langchain-openai in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (0.1.27)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (1.10.11)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (0.1.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (2.0.27)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (0.0.24)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: certifi in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: colorama in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'D:\\Code\\PromptEngineering_Langchain\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing by offering tools that automate tests and provide detailed reports on the results. It can also provide recommendations and suggestions for improving testing processes and addressing any issues that may arise during testing. Additionally, langsmith can offer test case management, issue tracking, and collaboration features that enhance overall efficiency and communication within the testing team. Overall, Langsmith aims to streamline the testing process and ensure that all components of the software are thoroughly examined for quality assurance.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can help with testing in several ways:\\n\\n1. Automated testing: Langsmith has the ability to generate test scripts and automate the execution of tests. This can speed up the testing process and improve test coverage.\\n\\n2. Synthetic testing: Langsmith can create synthetic data to simulate different testing scenarios. This can help uncover edge cases and ensure thorough testing.\\n\\n3. Load testing: Langsmith can also be used for load testing by simulating high levels of traffic on the system and monitoring performance metrics. This can help identify performance bottlenecks and ensure the system can handle expected traffic levels.\\n\\n4. Integration testing: By creating mock services or integrations with Langsmith, teams can perform end-to-end testing and verify the system behaves as expected when using different dependent components.\\n\\nOverall, Langsmith's versatility and adaptability make it a valuable tool for testing various aspects of a system and ensuring its reliability and robustness.\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of a ChatModel (and therefore, of this chain) is a message. \n",
    "# However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith is a powerful tool developed to assist with automating temporary data languages for building cohesive test environments. Testing can be time-consuming, tedious, and prone to human error, especially when manual efforts are required to set up the environment or generate test data. Langsmith simplifies these tasks by enabling testers to define and switch between various data languages that represent different scenarios, states, or configurations.\\n\\nUsing Langsmith can facilitate testing in the following ways:\\n\\n1. **Rapid Setup**: With Langsmith, testers can swiftly set up specific test data environments by simply selecting the intended data language, all in a programmatic and automated manner. This helps streamline the testing process and saves time by eliminating the need to manually configure environments.\\n\\n2. **Repeatability and Consistency**: Langsmith ensures that test configurations are consistent and reproducible across runs. Testers can establish distinct data languages for different test cases or scenarios, ensuring consistency and reliability in testing outcomes.\\n\\n3. **Data Variation**: By utilizing different data languages, testers can explore and cover a greater range of test scenarios, scaling the coverage and better preparing for potential edge cases within the application under test.\\n\\n4. **Improved Accuracy**: Automation through Langsmith minimizes manual errors in setting up and managing test data environments. This leads to precise testing results and helps in detecting defects earlier in the development lifecycle.\\n\\n5. **Enhanced Collaboration**: Teams using Langsmith can seamlessly share data language definitions among testers, developers, and stakeholders. This fosters collaboration and alignment within the testing process.\\n\\nIn essence, Langsmith plays a vital role in enhancing the efficiency, reliability, and effectiveness of testing activities. Integrating Langsmith into test automation practices can provide tangible benefits by empowering testers to manage test data more effectively, allowing for wider test coverage, and ultimately accelerating the overall testing process.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: content='The 7 wonders of the world are:\\n\\n1. Great Wall of China\\n2. Petra, Jordan\\n3. Christ the Redeemer, Brazil\\n4. Machu Picchu, Peru\\n5. Chichen Itza, Mexico\\n6. Colosseum, Italy\\n7. Taj Mahal, India'\n"
     ]
    }
   ],
   "source": [
    "# Basic Example of a Prompt.\n",
    "response = llm.invoke(\"What are the 7 wonders of the world?\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Templated Prompt: Human: List 3 cooking recipe ideas italian cuisine (name only).\n",
      "Response: content='1. Spaghetti Carbonara\\n2. Margherita Pizza\\n3. Osso Buco'\n"
     ]
    }
   ],
   "source": [
    "# Basic Example of a Prompt Template.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"List {n} cooking recipe ideas {cuisine} cuisine (name only).\"\n",
    ")\n",
    "prompt = prompt_template.format(n=3, cuisine=\"italian\")\n",
    "print(f\"Templated Prompt: {prompt}\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Parser\n",
    "- formatting responses\n",
    "- structured output\n",
    "\n",
    "ex: Movie\n",
    "- title: str\n",
    "- genre: list[str]\n",
    "- year: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(description=\"movie title\")\n",
    "    genre: list[str] = Field(description=\"movie genre\")\n",
    "    year: int = Field(description=\"year of relase\")\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"title\": {\"title\": \"Title\", \"description\": \"movie title\", \"type\": \"string\"}, \"genre\": {\"title\": \"Genre\", \"description\": \"movie genre\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"year\": {\"title\": \"Year\", \"description\": \"year of relase\", \"type\": \"integer\"}}, \"required\": [\"title\", \"genre\", \"year\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_template_text = \"\"\"\n",
    "Response with a movie recommendation based on the query:\\n\n",
    "{format_instructions}\\n\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\PromptEngineering_Langchain\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"Face/Off\",\n",
      "    \"genre\": [\"Action\", \"Crime\", \"Sci-Fi\"],\n",
      "    \"year\": 1997\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    prompt_template_text\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(query=\"A 90s movie with Nicolas Cage.\", format_instructions=format_instructions)\n",
    "text_output = llm.predict(prompt)\n",
    "print(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Face/Off' genre=['Action', 'Crime', 'Sci-Fi'] year=1997\n"
     ]
    }
   ],
   "source": [
    "parsed_output = parser.parse(str(text_output))\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Face/Off' genre=['Action', 'Crime', 'Sci-Fi'] year=1997\n"
     ]
    }
   ],
   "source": [
    "# Using LCEL\n",
    "chain = prompt_template | llm | parser\n",
    "response = chain.invoke({\"query\": \"A 90s movie with Nicolas Cage.\", \"format_instructions\": format_instructions})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chains\n",
    " Chaining prompts, input of a prompt depends on the output of another prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "Come up with a short plot synopsis summary (2-3 lines) for a {genre} movie.\n",
    "\"\"\"\n",
    "\n",
    "prompt_2 = \"\"\"\n",
    "Suggest 5 movie title ideas for this movie: \\n\\n {synopsis}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "p1_template = ChatPromptTemplate.from_template(prompt_1)\n",
    "chain_1 = LLMChain(llm=llm, prompt=p1_template, output_key=\"synopsis\")\n",
    "\n",
    "p2_template = ChatPromptTemplate.from_template(prompt_2)\n",
    "chain_2 = LLMChain(llm=llm, prompt=p2_template, output_key=\"titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\PromptEngineering_Langchain\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Output: {'genre': 'comedy', 'synopsis': \"When two brothers inherit an eccentric circus from their late father, they must navigate zany performers, outrageous challenges, and sibling rivalry to keep the business afloat and save their family legacy. With explosive humor and heartwarming moments, they discover there's more to their inheritance than just a bigtop tent.\", 'titles': '1. \"Circus Chaos: The Battle of the Brothers\"\\n2. \"Big Top Rivals: The Legacy of Laughs\"\\n3. \"Taming the Tent: A Circus Comedy\"\\n4. \"Sibling Spectacular: The Family Circus\"\\n5. \"Legacy Under the Big Top: Brothers in Comedy\"'}\n"
     ]
    }
   ],
   "source": [
    "complex_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2],\n",
    "    input_variables=[\"genre\"],\n",
    "    output_variables=[\"synopsis\", \"titles\"],\n",
    "    verbose=True,\n",
    ")\n",
    "output = complex_chain({\"genre\": \"comedy\"})\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numexpr\n",
      "  Downloading numexpr-2.9.0-cp310-cp310-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 KB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\code\\promptengineering_langchain\\env\\lib\\site-packages (from numexpr) (1.26.4)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'D:\\Code\\PromptEngineering_Langchain\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SerpAPIWrapper\n__root__\n  Did not find serpapi_api_key, please add an environment variable `SERPAPI_API_KEY` which contains it, or pass `serpapi_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m llm_math_chain \u001b[38;5;241m=\u001b[39m LLMMathChain\u001b[38;5;241m.\u001b[39mfrom_llm(llm\u001b[38;5;241m=\u001b[39mllm, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Set \"SERPAPI_API_KEY\" environment variable to your private key.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# https://serpapi.com/\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mSerpAPIWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m tools \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     Tool(\n\u001b[0;32m     19\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     ),\n\u001b[0;32m     28\u001b[0m ]\n\u001b[0;32m     30\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m     31\u001b[0m     [\n\u001b[0;32m     32\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     ]\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32md:\\Code\\PromptEngineering_Langchain\\env\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for SerpAPIWrapper\n__root__\n  Did not find serpapi_api_key, please add an environment variable `SERPAPI_API_KEY` which contains it, or pass `serpapi_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"search\",\n",
    "        description=\"Search for information Google.\",\n",
    "        func=search.run,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"calculator\",\n",
    "        description=\"Use this tool to calculate the difference between numbers.\",\n",
    "        func=llm_math_chain.run,\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "agent_schema = {\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"agent_scratchpad\": lambda x: format_to_openai_functions(x[\"intermediate_steps\"]),\n",
    "}\n",
    "\n",
    "agent = agent_schema | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"What is the population difference between ðŸ‡ºðŸ‡¸ and ðŸ‡¬ðŸ‡§?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
