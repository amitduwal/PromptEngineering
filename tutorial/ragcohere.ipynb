{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebc6ac2-6de0-4a4b-bc82-fc010abd7840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:51:59.460300Z",
     "iopub.status.busy": "2024-02-28T05:51:59.460024Z",
     "iopub.status.idle": "2024-02-28T05:52:02.518616Z",
     "shell.execute_reply": "2024-02-28T05:52:02.517773Z",
     "shell.execute_reply.started": "2024-02-28T05:51:59.460276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain cohere bs4 faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49517667-ef4e-451e-9e99-86f910f46b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:52:02.520282Z",
     "iopub.status.busy": "2024-02-28T05:52:02.520046Z",
     "iopub.status.idle": "2024-02-28T05:52:05.368421Z",
     "shell.execute_reply": "2024-02-28T05:52:05.367675Z",
     "shell.execute_reply.started": "2024-02-28T05:52:02.520254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q typing-inspect==0.8.0 typing_extensions==4.5.0 pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8e6d48-07eb-415e-a7d6-ed921cc8f62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:52:05.369679Z",
     "iopub.status.busy": "2024-02-28T05:52:05.369466Z",
     "iopub.status.idle": "2024-02-28T05:52:06.020864Z",
     "shell.execute_reply": "2024-02-28T05:52:06.020152Z",
     "shell.execute_reply.started": "2024-02-28T05:52:05.369657Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatCohere\n",
    "from langchain_community.embeddings import CohereEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b95b6b7-fa0d-4bf2-907f-478abf5126f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:52:06.022889Z",
     "iopub.status.busy": "2024-02-28T05:52:06.022606Z",
     "iopub.status.idle": "2024-02-28T05:52:06.111720Z",
     "shell.execute_reply": "2024-02-28T05:52:06.110814Z",
     "shell.execute_reply.started": "2024-02-28T05:52:06.022868Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatCohere(cohere_api_key=\"nMfqYUZJoPhJ3uI6aJq573MEHa9MsXxtSUGRVw5f\")\n",
    "embeddings = CohereEmbeddings(cohere_api_key=\"nMfqYUZJoPhJ3uI6aJq573MEHa9MsXxtSUGRVw5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15aca0ce-defd-4f80-b929-2488a36568fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:52:06.112842Z",
     "iopub.status.busy": "2024-02-28T05:52:06.112584Z",
     "iopub.status.idle": "2024-02-28T05:52:06.425556Z",
     "shell.execute_reply": "2024-02-28T05:52:06.424783Z",
     "shell.execute_reply.started": "2024-02-28T05:52:06.112822Z"
    }
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://jalammar.github.io/illustrated-transformer/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74d02e5-fb1c-473a-89dc-f8a1d42b20f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:52:06.426915Z",
     "iopub.status.busy": "2024-02-28T05:52:06.426630Z",
     "iopub.status.idle": "2024-02-28T05:52:07.896040Z",
     "shell.execute_reply": "2024-02-28T05:52:07.895373Z",
     "shell.execute_reply.started": "2024-02-28T05:52:06.426889Z"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector_store = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c806fe2d-8f73-4a6e-a7c0-f22cf38e898a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:52:07.897553Z",
     "iopub.status.busy": "2024-02-28T05:52:07.896953Z",
     "iopub.status.idle": "2024-02-28T05:52:08.341001Z",
     "shell.execute_reply": "2024-02-28T05:52:08.340439Z",
     "shell.execute_reply.started": "2024-02-28T05:52:07.897528Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf9af547-b35d-47cd-a057-bdc4419d4eff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:58:44.691319Z",
     "iopub.status.busy": "2024-02-28T05:58:44.690423Z",
     "iopub.status.idle": "2024-02-28T05:58:44.695435Z",
     "shell.execute_reply": "2024-02-28T05:58:44.694754Z",
     "shell.execute_reply.started": "2024-02-28T05:58:44.691284Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd84923b-9e39-4225-8ed3-989cb33d7760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:58:45.769307Z",
     "iopub.status.busy": "2024-02-28T05:58:45.768608Z",
     "iopub.status.idle": "2024-02-28T05:58:50.128125Z",
     "shell.execute_reply": "2024-02-28T05:58:50.127404Z",
     "shell.execute_reply.started": "2024-02-28T05:58:45.769281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is a transformer?',\n",
       " 'context': [Document(page_content='Go Forth And Transform\\nI hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:', metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n', 'language': 'No language found.'}),\n",
       "  Document(page_content='Acknowledgements\\nThanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.\\nPlease hit me up on Twitter for any corrections or feedback.\\n\\n\\n    Written on June 27, 2018\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\nSubscribe to get notified about upcoming posts by email\\n\\nEmail Address \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\\n\\n\\nAttribution example:\\n\\nAlammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/\\n\\nNote: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.', metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n', 'language': 'No language found.'}),\n",
       "  Document(page_content='The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:', metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n', 'language': 'No language found.'}),\n",
       "  Document(page_content='The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJay Alammar\\nVisualizing machine learning one concept at a time.@JayAlammar on Twitter. YouTube Channel\\n\\n\\nBlog\\nAbout\\n\\n\\n\\n\\n\\n\\nThe Illustrated Transformer\\n\\nDiscussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post', metadata={'source': 'https://jalammar.github.io/illustrated-transformer/', 'title': 'The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.', 'description': 'Discussions:\\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\\n\\n\\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\\n\\nWatch: MIT’s Deep Learning State of the Art lecture referencing this post\\n\\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and others\\n\\nIn the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\\n\\nThe Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\\n\\n2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:\\n\\n\\n\\n\\nA High-Level Look\\nLet’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\\n\\n\\n  \\n\\n\\n', 'language': 'No language found.'})],\n",
       " 'answer': 'Transformers are a type of machine-learning model behind NLP tasks. They rely entirely on attention mechanisms to handle long-range dependencies and context by attending to different parts of the input sequence. This makes them ideal for tasks involving sequential or time-series data like NLP tasks. They are called \"transformers\" because they use the same weight matrix to transform different elements of the input sequence.\\n\\nWould you like me to go into more detail about transformers?'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What is a transformer?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b60105-2469-4ee9-89a7-8758921bc14d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T05:58:50.130350Z",
     "iopub.status.busy": "2024-02-28T05:58:50.129385Z",
     "iopub.status.idle": "2024-02-28T05:58:50.135340Z",
     "shell.execute_reply": "2024-02-28T05:58:50.134580Z",
     "shell.execute_reply.started": "2024-02-28T05:58:50.130317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers are a type of machine-learning model behind NLP tasks. They rely entirely on attention mechanisms to handle long-range dependencies and context by attending to different parts of the input sequence. This makes them ideal for tasks involving sequential or time-series data like NLP tasks. They are called \"transformers\" because they use the same weight matrix to transform different elements of the input sequence.\\n\\nWould you like me to go into more detail about transformers?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d94c8b-1a06-4d5c-bd6c-576462924eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f742e-2763-43ff-9bc8-f450907c4647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8e997-07e6-4490-9e09-6218f58c5a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
